# 系统架构

本文档详细描述了Vent-Scraper的系统架构、项目结构和各组件之间的关系。

## 项目结构

```
vent-scraper/
├── .github/workflows/   # GitHub Actions工作流配置
│   └── daily-scraper.yml  # 每日自动爬取任务
├── data/                # 爬取的数据存储
│   ├── <source_id>/     # 每个源的数据目录
│   │   ├── week_YYYY-MM-DD.json  # 按周存储的数据文件
│   │   └── archive/    # 归档的历史数据
│   └── sources.json     # 数据源索引文件
├── icons/               # 数据源图标存储
│   └── <source_id>.png  # 各源的图标文件
├── docs/                # 项目文档
├── logs/                # 日志文件
├── src/                 # 源代码
│   ├── main.py          # 主入口文件
│   ├── scrapers/        # 爬虫实现
│   │   ├── __init__.py
│   │   ├── base_scraper.py  # 爬虫基类
│   │   ├── jandan_scraper.py  # 煎蛋网爬虫
│   │   └── registry.py  # 爬虫注册中心
│   └── utils/           # 工具函数
│       ├── __init__.py
│       ├── http_utils.py  # HTTP请求工具
│       └── storage_utils.py  # 数据存储工具
└── README.md            # 项目说明
```

## 架构概述

Vent-Scraper采用模块化设计，主要分为以下几个核心组件：

1. **命令行接口层**：提供用户交互界面
2. **爬虫管理层**：管理和协调各个爬虫的运行
3. **爬虫实现层**：实现特定网站的爬取逻辑
4. **工具层**：提供HTTP请求、数据存储等通用功能
5. **自动化层**：通过GitHub Actions实现自动化运行

## 模块关系图

```
main.py <-- 命令行入口
  |
  +--> scrapers/registry.py <-- 爬虫注册和管理
  |      |
  |      +--> base_scraper.py <-- 爬虫基类
  |      |      |
  |      |      +--> utils/http_utils.py <-- HTTP请求处理
  |      |
  |      +--> jandan_scraper.py <-- 煎蛋网爬虫实现
  |
  +--> utils/storage_utils.py <-- 数据存储管理
```

## 数据流

1. 用户通过命令行接口发起爬取请求
2. 主程序调用爬虫注册中心获取相应的爬虫实例
3. 爬虫实例发送网络请求获取网页数据
4. 爬虫解析网页数据并转换为标准格式
5. 数据通过存储工具按周保存到本地文件系统，已存在的内容会进行增量更新
6. 更新数据源索引，记录爬取状态

## 组件详解

### 1. 命令行接口 (main.py)

- 解析命令行参数
- 路由请求到相应的处理函数
- 提供用户友好的反馈信息
- 管理数据源图标

### 2. 爬虫注册中心 (registry.py)

- 维护已注册爬虫的映射表
- 提供爬虫创建和调用的统一接口
- 处理爬虫运行时的异常情况

### 3. 爬虫基类 (base_scraper.py)

- 定义爬虫的通用接口和行为
- 提供网络请求、数据处理等基础功能
- 实现数据增量保存和索引更新逻辑

### 4. 具体爬虫实现 (jandan_scraper.py等)

- 实现特定网站的爬取逻辑
- 处理网页解析和数据提取
- 将数据转换为标准格式

### 5. HTTP工具 (http_utils.py)

- 封装网络请求功能
- 实现请求缓存、重试、限流等机制
- 处理常见的网络异常情况

### 6. 存储工具 (storage_utils.py)

- 管理按周存储的数据
- 提供增量更新功能
- 提供数据归档功能
- 维护数据源索引

### 7. 自动化配置 (daily-scraper.yml)

- 定义每日自动运行的工作流
- 配置代码检出、环境设置等步骤
- 处理数据提交和归档操作

## 数据存储结构

数据以JSON格式按周存储在本地文件系统中：

- `data/<source_id>/week_YYYY-MM-DD.json`：每周的爬取数据（周一日期）
- `data/<source_id>/archive/`：归档的历史数据
- `data/sources.json`：所有数据源的索引信息
- `icons/<source_id>.png`：数据源图标

每个数据文件的基本结构：

```json
{
  "meta": {
    "source_id": "数据源ID",
    "source_name": "数据源名称",
    "timestamp": "首次爬取时间",
    "updated_at": "最后更新时间",
    "last_fetch_date": "最后爬取日期",
    "version": "数据版本",
    "icon": "图标文件名"
  },
  "data": [
    {
      "id": "唯一标识符",
      "content": "内容",
      "created_at": "创建时间",
      "author": "作者",
      "metrics": {
        "likes": 点赞数,
        "comments": 评论数
      },
      "tucao": [
        {
          "id": "评论ID",
          "content": "评论内容",
          "author": "评论作者",
          "created_at": "评论时间"
        }
      ],
      "extra_data": {
        // 特定于来源的额外数据
      }
    }
  ]
}
```

## 增量更新机制

系统采用按周存储数据，并实现了增量更新机制：

1. 每次爬取时，根据当前日期计算所属的周（以周一为起始日期）
2. 如果该周数据文件已存在，读取现有数据并进行合并
3. 对于已存在的内容，更新其最新指标和新增评论
4. 对于新增内容，直接添加到数据文件中
5. 更新文件的元数据信息，如最后更新时间等

## 扩展性设计

系统设计考虑了以下几个方面的扩展性：

1. **新爬虫添加**：只需创建新的爬虫类并在注册中心注册
2. **数据处理流程**：可以通过继承和重写基类方法自定义处理流程
3. **存储方式**：存储机制可以根据需要扩展到数据库或云存储
4. **命令行功能**：可以通过添加新的子命令扩展功能

## 安全性考虑

- 网络请求使用随机延迟和用户代理，避免被目标网站封禁
- 错误处理和日志记录，便于排查问题
- 数据归档机制，防止数据过度积累

## 性能优化

- HTTP请求缓存机制，减少重复请求
- 请求限流，避免对目标服务器造成过大压力
- 异常重试机制，提高爬取成功率 
- 增量更新，减少存储空间并提高爬取效率 